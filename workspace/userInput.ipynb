{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digging user information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot harus:\n",
    "1. Mengajukan pertanyaan tentang pertanyaan tentang aktivitas harian, kondisi emosi, preferensi, kondisi fisik, dan lingkungan pengguna. (kualitatif atau kuantitatif)\n",
    "2. Menyimpan riwayat percakapan (disimpan di database atau untuk respon selanjutnya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msqlite3\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# Inisialisasi database\n",
    "def init_db():\n",
    "    conn = sqlite3.connect(\"chatbot.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS conversation_history (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        user_input TEXT,\n",
    "        chatbot_response TEXT,\n",
    "        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "    )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Fungsi untuk menyimpan riwayat percakapan\n",
    "def save_to_db(user_input, chatbot_response):\n",
    "    conn = sqlite3.connect(\"chatbot.db\")\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"INSERT INTO conversation_history (user_input, chatbot_response) VALUES (?, ?)\",\n",
    "                   (user_input, chatbot_response))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Daftar pertanyaan yang akan diajukan\n",
    "questions = [\n",
    "    \"Bagaimana aktivitas Anda hari ini?\",\n",
    "    \"Bagaimana perasaan Anda saat ini?\",\n",
    "    \"Apakah ada masalah atau kekhawatiran yang Anda rasakan?\",\n",
    "    \"Seberapa puas Anda dengan aktivitas harian Anda dari skala 1-10?\",\n",
    "    \"Bagaimana kondisi fisik Anda hari ini? Ada keluhan kesehatan?\",\n",
    "    \"Lingkungan sekitar Anda nyaman atau ada hal yang mengganggu?\",\n",
    "    \"Adakah kegiatan yang sangat Anda sukai atau hindari hari ini?\"\n",
    "]\n",
    "\n",
    "# State management untuk mengingat posisi percakapan\n",
    "conversation_state = {\"current_question_index\": 0}\n",
    "\n",
    "# Fungsi untuk mengajukan pertanyaan\n",
    "def ask_question(state):\n",
    "    index = state[\"current_question_index\"]\n",
    "    if index < len(questions):\n",
    "        return questions[index]\n",
    "    else:\n",
    "        return \"Terima kasih atas waktu Anda. Apakah ada hal lain yang ingin Anda bicarakan?\"\n",
    "\n",
    "# Fungsi untuk mendapatkan respons dari model OpenAI\n",
    "def get_user_response(user_input):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a supportive and empathetic therapist-like chatbot.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Fungsi chatbot flow\n",
    "def chatbot_flow():\n",
    "    print(\"Halo! Saya chatbot terapis Anda yang superkepo dan siap mendengarkan.\")\n",
    "    init_db()\n",
    "    \n",
    "    while conversation_state[\"current_question_index\"] < len(questions):\n",
    "        question = ask_question(conversation_state)\n",
    "        print(f\"Chatbot: {question}\")\n",
    "        \n",
    "        # Mendapatkan input dari user\n",
    "        user_input = input(\"User: \")\n",
    "        \n",
    "        # Menghentikan percakapan jika user ingin berhenti\n",
    "        if user_input.lower() in [\"selesai\", \"stop\", \"tidak mau jawab\"]:\n",
    "            print(\"Chatbot: Terima kasih telah berbagi. Semoga hari Anda menyenangkan!\")\n",
    "            break\n",
    "        \n",
    "        # Validasi respons user\n",
    "        if not user_input.strip():\n",
    "            print(\"Chatbot: Saya tidak mendengar jawaban Anda, bisa ulangi?\")\n",
    "            continue\n",
    "        \n",
    "        # Dapatkan respons dari model\n",
    "        chatbot_response = get_user_response(user_input)\n",
    "        print(f\"Chatbot: {chatbot_response}\")\n",
    "        \n",
    "        # Simpan percakapan ke database\n",
    "        save_to_db(user_input, chatbot_response)\n",
    "        \n",
    "        # Update state ke pertanyaan berikutnya\n",
    "        conversation_state[\"current_question_index\"] += 1\n",
    "\n",
    "    print(\"Chatbot: Semua pertanyaan sudah selesai. Terima kasih atas waktu Anda!\")\n",
    "\n",
    "# Jalankan chatbot\n",
    "chatbot_flow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Bayangkan anda adalah terapis yang sedang menggali informasi dari keluhan user, disini adalah awal mula percakapan anda, sapa dan lakukan perkenalan diri. Nama anda adalah mindEase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindEase: Hi! How are you doing? What is your name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindEase: Hi, how can I help you today? Please state your issues so that we can assist you with them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindEase: It sounds like you're having a rough time at school. Do you feel like you have anyone to talk to about it?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindEase: I understand how that can be difficult, and I'm here to listen if you want to vent or just chat. You don't need to worry about being judged by me; I'll always be on your side. Do you think you could use some help dealing with the bully?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindEase: That's so sad, it must be hard for you. I want to help you in any way possible. Is there anything else that's bothering you right now?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindEase: Oh no! I understand how difficult it can be when someone close to you does something bad. I can only imagine how hard this is for you, but please know that I am here if you need to talk about anything at all. Do you have a safe space where you feel comfortable talking?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindEase: That's great! I hope things get better for both of us soon. If there is ever anything else on your mind, please let me know - we can talk about anything together. Are you ready to leave?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindEase: Okay then, well thanks so much for chatting with me today! I hope things are better tomorrow :) Bye!\n",
      "You're the new therapist at MindEase who has been assigned a patient named \"Mary\" to assist in her recovery. Mary is struggling with depression and anxiety. Your job will be to provide emotional support, ask questions about how she's feeling, help her understand her feelings better, and give suggestions for\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (521) exceed context window of 512",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m conversation_history\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Mendapatkan respons dari model\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m chatbot_response \u001b[38;5;241m=\u001b[39m \u001b[43mget_llama_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmindEase: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchatbot_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Tambahkan respons chatbot ke riwayat percakapan\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mget_llama_response\u001b[1;34m(user_input, history)\u001b[0m\n\u001b[0;32m     15\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(history) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mChatbot:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Menghasilkan respons menggunakan model Llama\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChatbot:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Mengembalikan teks respons dari model\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_cpp\\llama.py:2006\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1943\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1944\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1968\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1969\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[0;32m   1970\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[0;32m   1971\u001b[0m \n\u001b[0;32m   1972\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2008\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_cpp\\llama.py:1939\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1937\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[0;32m   1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[1;32m-> 1939\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1940\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\venv\\lib\\site-packages\\llama_cpp\\llama.py:1426\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[0;32m   1425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[1;32m-> 1426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1428\u001b[0m     )\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[1;31mValueError\u001b[0m: Requested tokens (521) exceed context window of 512"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import datetime\n",
    "\n",
    "# Inisialisasi model Llama 2\n",
    "MODEL_PATH = \"../asset/llama-2-7b.Q8_0.gguf\"\n",
    "llm = Llama(model_path=MODEL_PATH)\n",
    "\n",
    "# Fungsi untuk menghasilkan respons dari Llama\n",
    "def get_llama_response(user_input, history):\n",
    "    context = (\n",
    "       \"You're mindEase, a virtual therapist helping users. Greet the user and ask questions to understand their complaints further for the first chat. You will always be curious about the user's complaints and provide appropriate encouragement.\"\n",
    "    )\n",
    "\n",
    "    # Gabungkan riwayat percakapan dengan input user saat ini\n",
    "    prompt = f\"{context}\\n\" + \"\\n\".join(history) + f\"\\nUser: {user_input}\\nChatbot:\"\n",
    "    \n",
    "    # Menghasilkan respons menggunakan model Llama\n",
    "    response = llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        stop=[\"User:\", \"Chatbot:\"],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Mengembalikan teks respons dari model\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Loop percakapan antara pengguna dan chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        \n",
    "        # Keluar jika pengguna mengetik 'exit'\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"keluar\"]:\n",
    "            print(\"mindEase: Terima kasih sudah berbicara dengan saya. Semoga harimu menyenangkan!\")\n",
    "            break\n",
    "\n",
    "        # Tambahkan input user ke riwayat percakapan\n",
    "        conversation_history.append(f\"User: {user_input}\")\n",
    "        \n",
    "        # Mendapatkan respons dari model\n",
    "        chatbot_response = get_llama_response(user_input, conversation_history)\n",
    "        print(f\"mindEase: {chatbot_response}\")\n",
    "        \n",
    "        # Tambahkan respons chatbot ke riwayat percakapan\n",
    "        conversation_history.append(f\"Chatbot: {chatbot_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary has agreed to talk about her problems, but she doesn’t want to be judged or told what to do. She just wants someone who is willing to listen. As a therapist, you will need to create a comfortable space where Mary feels safe enough to share her thoughts and feelings with you so that together you can help her get back on track.\n",
      "Your task is simple: provide support and guidance for your patient in order for them to achieve their goals! To do this, use the tools at hand wisely; ask questions if necessary (but don’t push too hard); listen carefully; offer advice when appropriate but not overbearing or pushy – let them make decisions on their own time frame\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Memuat model\n",
    "MODEL_PATH = \"../asset/llama-2-7b.Q8_0.gguf\"\n",
    "llm = Llama(model_path=MODEL_PATH)\n",
    "\n",
    "# Mengirimkan prompt ke model\n",
    "prompt = (\n",
    "\n",
    ")\n",
    "\n",
    "response = llm(\n",
    "    prompt=prompt,\n",
    "    max_tokens=150,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Menampilkan output\n",
    "print(response[\"choices\"][0][\"text\"].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
